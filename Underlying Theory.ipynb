{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    "Supervised learning algorithm introduced by Breiman et. al (1984) in which the general idea is to recursively split the covariate space into homogeneous and non-overlapping partitions within each the prediction of the response is constant. Figure below summarize graphically how a Decision Tree works.\n",
    "\n",
    "<center><img src = \"DT.png\" width=\"600\" height=\"520\"/></center>\n",
    "\n",
    "Suppose to have a sample $X_{1},..,X_{n}$ and each sample point is a p-variate random variable $X_{i1},..,X_{ip}$ with $i=1,..,n$. In the initial setting exists only a single region $R$ which is equivalent to the covariate space $\\chi$. The goal is to find a splitting point, defined as a couple $(j,s_{1})$ where $j = 1,..,p$ is one of the $p$ covariates and $s_{1}$ is one of its possible values in the sample, that partitionate the covariate space into two non-overlapping regions. Mathematically speaking:\n",
    "\n",
    "$$\n",
    "    (j,s_{1}): R_{1}=\\big\\{(X_{1},...X_{p})\\in \\chi : X_{j}\\leq s_{1}\\}, \\hspace{0.5cm} R_{2}=\\{(X_{1},...X_{p})\\in \\chi : X_{j}>s_{1}\\big\\} \n",
    "$$\n",
    "\n",
    "The best splitting point is defined as that point that provides the best improvement of an impurity measure $\\phi$ defined as a measure of the degree of heterogeneity in the class distribution. Possible choices of $\\phi$ are:\n",
    "* **Gini Index**\n",
    "$$\n",
    "    G = \\sum_{k=1}^{K}\\hat{p}_{mk}(1-\\hat{p}_{mk}) = 1 - \\sum_{k=1}^{K}\\hat{p}_{mk}^{2}\n",
    "$$\n",
    "* **Entropy**\n",
    "$$\n",
    "    D = -\\sum_{k=1}^{K}\\hat{p}_{mk}\\log{\\hat{p}_{mk}}\n",
    "$$\n",
    "where $\\hat{p}_{mk}$ is the proportion of observations that are from the $k$-th class in the $m$-th region. Looking at the Gini index, we notice that it can be considered as a sum of Bernoulli variances over the $k$ classes; it's easy to see that the minimum value is observed when the $\\hat{p}_{mk}$'s have extreme values (i.e. when the partition is mainly represented of observations belonging to one class). Once the splitting has occurred, a proportion $p_{1}$ of the observations is sent to the first region, $p_{2}$ to the second and the impurity measure for the two regions $\\phi_{R_{1}}$,$\\phi_{R_{2}}$ is recorded. Defined $\\phi_{R}$ the overall impurity measure, the change in impurity is given by:\n",
    "\n",
    "$$\n",
    "    \\Delta i = \\phi_{R} - p_{1}\\phi_{R_{1}} - p_{2}\\phi_{R_{2}}\n",
    "$$\n",
    "so the best splitting point is obtained via minimization:\n",
    "\n",
    "$$\n",
    "    \\min_{j,s1} \\big[p_{1}\\phi_{R_{1}} + p_{2}\\phi_{R_{2}}\\big]\n",
    "$$\n",
    "The procedure is applied recursively on both the partitions created until some termination criterion is satisfied; generally we consider a minimum number of observations within each region (or leaf node) or a maximum depth of the tree. This is done since a very depth tree or a tree in which each leaf node contains only few observations, is more likely to overfit our data providing lack in generalization which will lead in poor predictive performance. Once all the partitions are being defined, the prediction of the response is done by taking the most occuring class in that partition. \n",
    "\n",
    "## Ensemble methods\n",
    "\n",
    "Decision trees are very simply to apply and to communicate to people: they represent graphically how people make decisions! The cost of simplicity is a lower predictive accuracy when compared to other classification algorithms due to the fact that they more likely will overfit our data. However the performance can be drastically improved with **ensemble methods**, defined as approaches that combines many $\\textit{weak \\hspace{0.07cm} learners}$ defined as such since they provide poor predictions on their own. Ensemble methods are not properly related to decision trees, since the concept is a general idea. As a drawback, these methods are no longer easily interpretable and the decision path is much less transparent respect to a DT. Furthermore, in RF is not possible to depict the role of the variables in exaplining or causing; it's only possible to measure the impact of each predictor into the predictive process *(variables importance)*.The following are two of the ensemble models which use decision tree as weak learner.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "Bagging, short for Bootstrap aggregation, is a technique which combines many classification trees in order to improve the prediction accuracy. In the following we present the algorithm of bagging.\n",
    "\n",
    "* Given a training set, a number $B$ of new sets are obtained via Bootstrap i.e by resampling with replacement the given set. \n",
    "* For each Bootstrap sample a very deep and not pruned classification tree is fitted: it will have low bias (performance on the training set) and high variance. \n",
    "* Define $\\hat{f}^{*b}(x)$ the prediction for a new observation $x$ from a single tree. The bagging prediction $\\hat{f}_{bag}(x)$ is given by the *majority vote rule*\n",
    "$$\n",
    "    \\hat{f}_{bag}(x) = \\arg \\max_{k} \\sum_{b=1}^{B}I_{\\{\\hat{f}^{*b}(x)=k)\\}}\n",
    "$$\n",
    "\n",
    "##### *Out Of Bag Test Error Estimation*\n",
    "Bagging provides also a good estimate of the test error; in particular, it can been shown that on average each tree is fitted on the $\\frac{2}{3}$ of the entire sample: the remaining $\\frac{1}{3}$ are called Out Of Bag (OOB) observations; so given a single observation $i$, there will be about $\\frac{B}{3}$ trees for which it is an OOB. Using them for making the predection and taking the majority vote, the estimate error is computed. Repeating this for all the observations in the sample one can compute the total estimate error and so provide an estimate of the test error.\n",
    "\n",
    "### Random Forest\n",
    "The amount of variance reduction in bagging is reduced if the trees are highly correlated; this phenomena occurs when the trees share similar structures. Random Forest is a bagging procedure, with the only difference that each tree is forced to consider only a subset $m$ of predictors randomly chosen; in classification tasks generally $m=\\sqrt{p}$. This provides a simple tool to decorrelate the trees and improve the predictive accuracy. The idea beyond is that if there's a variable with an high impact on the predictions, the trees more likely will make the first split over that variable, and so the tree structures will be similar; by randomly choosing a subset of variables we give chances also to the less important variables!\n",
    "\n",
    "\n",
    "## Learning algorithms with imbalanced data\n",
    "Considering a binary classification problem (but is also true for multi class classification), a dataset is said *imbalanced* when it contains lots of samples beloging to one class which is called *majority class* and few samples in the other, *minority class*. The problem here is that many classification algorithms assumes that there is an approximally equal number of samples in each class; this is also the reason for which *accuracy* is the most used metric to evaluate the predictive performances. In imbalanced data framework, each Bootstrap sample more likely will contain mainly observations in the *majority class* and so the algorithm will learn quite perfectly how to predict them, but with poor predictive performance on the *minority class*. \\\n",
    "One way to solve this issue is to use **Resampling techniques** i.e. techniques which aim is to define a new training set by sampling from the imbalanced one; these are divided into:\n",
    "* Oversampling\n",
    "* Undersampling\n",
    "* Hybrid \n",
    "\n",
    "A lot of literaure and techniques exist on the topic, but we will focus only on three of them, one for each category.\n",
    "\n",
    "### Random Under Sampling (RUS)\n",
    "This technique is extremely easy: it simply randomly remove observations from the majority class until the set is balanced. It can be applied in presence of a great number of observations. However, when the dataset is highly imbalanced, there will remain too few samples to train the model; futhermore, we loose a relevant amount of data, which is never a good idea. \n",
    "\n",
    "### SMOTE \n",
    "Introduced by Chawla et. al (2002), SMOTE is a data augmentation technique which generates new synthetic samples by resampling the minority class until the new training set is balanced. Oversampling techniques such as ROS (Random Oversampling) are criticated since they could lead to overfitting. However this is not the case of SMOTE because insead of randomly copy existing sample, it generates new ones by taking into account the nearest neighbours. \n",
    "\n",
    "Taking one sample $X$ we compute the *k-nearest neighbours* $T_{1},..,T_{k}$  where *k* is the number of neighbours that must be set a priori (generally 5). We then take only $N<K$ of them where $N$ depends on the amount of oversampling desired and for each one $T_{i}$, compute the difference between its feature vector and the feature vector of the minority sample $X$. This difference is then multiplied by a random number $\\delta\\in(0,1)$ and then added to the feature vector of $X$. The new synthetic samples $X_{i}$ are obtained as follows.\n",
    "\n",
    "$$\n",
    "    X_{i} = X + \\delta(T_{i}-X) \\hspace{0.4cm} i=1,..,N\n",
    "$$\n",
    "\n",
    "The amount of new synthetic samples depends on the imbalance degree and so to the ratio between number of minority samples and majority ones. For example, if the ratio between minority and majority samples is 0.5, we will consider all the minority samples and for each of them we will consider only 2 of the *k* neighbours and create 2 new synthetic samples as described above.\n",
    "\n",
    "### SMOTE + RUS\n",
    "In the original paper, it has been demonstrated that SMOTE provide better performances when combined with Random Undersampling. The idea is to firstly oversample the minority class until a desired ratio is achieved, then undersample the majority class in order to balance the data. In this way we fix the issues induced by RUS.\n",
    "## References\n",
    "\n",
    "**Decision Trees and Random Forests** \n",
    "* James G., Witten D., Hastie T., Tibshirani R. (2013) Tree-Based Methods. In: An Introduction to Statistical Learning. Springer Texts in Statistics, vol 103. Springer, New York, NY. https://doi.org/10.1007/978-1-4614-7138-7_8 \\\n",
    "* K. A. Grajski, L. Breiman, G. V. Di Prisco and W. J. Freeman, \"Classification of EEG Spatial Patterns with a Tree-Structured Methodology: CART,\" in IEEE Transactions on Biomedical Engineering, vol. BME-33, no. 12, pp. 1076-1086, Dec. 1986, doi: 10.1109/TBME.1986.325684.\n",
    "\n",
    "**Imbalanced dataset learning**\n",
    "\n",
    "* Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002): 321-357.\n",
    "* Tan, Xiaopeng, et al. \"Wireless sensor networks intrusion detection based on SMOTE and the random forest algorithm.\" Sensors 19.1 (2019): 203.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
